# llm-eval-pro
Evaluate and benchmark Large Language Models (LLMs) for factuality, hallucination, and retrieval accuracy â€” with visual dashboards and reproducible metrics.
