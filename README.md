# LLM Eval Pro

Evaluate and benchmark Large Language Models (LLMs) for factuality, hallucination, and retrieval accuracy.

##  Overview
**LLM Eval Pro** provides a modular framework for comparing and validating LLM performance across different models and datasets.  
It supports custom evaluation metrics, visual dashboards, and automated reports.

###  Features
- Compare multiple LLMs (OpenAI, Claude, Gemini, etc.)
- Measure hallucination and retrieval accuracy
- Track metrics such as BLEU, BERTScore, recall, and precision
- Visualize results with Matplotlib or W&B dashboards
- Plug-and-play architecture for RAG and agent pipelines

### ⚙️ Tech Stack
- **Languages:** Python, SQL  
- **Frameworks:** LangChain, Pandas, Matplotlib, W&B  
- **Infra:** Docker (optional)  

###  Project Structure
