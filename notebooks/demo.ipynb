{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc9b5d4",
   "metadata": {},
   "source": [
    "# ðŸ§  llm-eval-pro â€“ Demo Notebook\n",
    "\n",
    "This notebook runs the full evaluation pipeline:\n",
    "\n",
    "1. Load config & dataset  \n",
    "2. Run one or more models  \n",
    "3. Compute metrics  \n",
    "4. Inspect results and plots  \n",
    "\n",
    "The default config uses a local `dummy-echo` model so you can run this\n",
    "without any API keys. Later you can plug in OpenAI or other providers\n",
    "via `configs/eval_config.yaml`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ecbff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# If you run this from the `notebooks/` directory, this points to project root.\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "\n",
    "CONFIG_PATH = PROJECT_ROOT / \"configs\" / \"eval_config.yaml\"\n",
    "DATASET_PATH = PROJECT_ROOT / \"data\" / \"samples.json\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\"\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Config path:\", CONFIG_PATH)\n",
    "print(\"Dataset path:\", DATASET_PATH)\n",
    "print(\"Output dir:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d561933",
   "metadata": {},
   "source": [
    "## 1. Inspect dataset\n",
    "\n",
    "We start by looking at the evaluation dataset â€“ this is what each model\n",
    "will be asked to answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabc1308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with DATASET_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(data)} examples\")\n",
    "pd.DataFrame(data).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a612a92",
   "metadata": {},
   "source": [
    "## 2. Run evaluation\n",
    "\n",
    "Here we run the main evaluation loop.  \n",
    "This will:\n",
    "\n",
    "- load `configs/eval_config.yaml`\n",
    "- evaluate each configured model\n",
    "- compute metrics per example\n",
    "- write CSVs under `outputs/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ba694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.compare_models import run_evaluation\n",
    "\n",
    "# Make sure outputs directory exists, then run\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "run_evaluation(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd84c86",
   "metadata": {},
   "source": [
    "## 3. Per-example metrics\n",
    "\n",
    "We can now inspect the per-example metrics to understand where a model\n",
    "is doing well or failing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d6f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_example_path = OUTPUT_DIR / \"per_example_metrics.csv\"\n",
    "per_example_df = pd.read_csv(per_example_path)\n",
    "\n",
    "print(per_example_df.shape)\n",
    "per_example_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d004ab",
   "metadata": {},
   "source": [
    "## 4. Aggregated metrics\n",
    "\n",
    "For model comparison, we care about metrics aggregated across the whole\n",
    "evaluation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca74c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_path = OUTPUT_DIR / \"aggregated_metrics.csv\"\n",
    "aggregated_df = pd.read_csv(aggregated_path)\n",
    "\n",
    "aggregated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f436033",
   "metadata": {},
   "source": [
    "## 5. Visualize model comparison\n",
    "\n",
    "We generate simple bar charts comparing models across each metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28910370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.visualize import visualize_aggregated\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Regenerate charts from aggregated metrics\n",
    "visualize_aggregated(aggregated_path)\n",
    "\n",
    "# Also display metrics inline in the notebook\n",
    "plt.style.use(\"default\")\n",
    "\n",
    "for metric in [c for c in aggregated_df.columns if c != \"model\"]:\n",
    "    ax = aggregated_df.plot(\n",
    "        x=\"model\",\n",
    "        y=metric,\n",
    "        kind=\"bar\",\n",
    "        legend=False,\n",
    "        title=f\"Model comparison â€“ {metric}\",\n",
    "    )\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_xlabel(\"model\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
